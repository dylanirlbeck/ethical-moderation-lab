{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76939895",
   "metadata": {},
   "source": [
    "# Ethical Moderation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77599c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Content Moderation is a subtopic within Computer Science ethics that has gained traction since the rise of popular social media platforms. Successful platforms such as Twitter, Reddit, and Quora have produced a space where everyone is allowed to voice their opinions on any topic.\n",
    "\n",
    "Today, we will explore content moderation with a Computer Science perspective, and the delicate issues that arise from too much or too little moderation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ede49f",
   "metadata": {},
   "source": [
    "## Lab Background\n",
    "__Sport-It__ is a popular social media platform which is known for its variety of communities, all of which relate to a specific sport. For example, there is a Sport-It community for the NBA, NFL, NHL, and many more. \n",
    "\n",
    "The moderators at Sport-It have decided that they want to create an environment where users only post about sports, and not controversial topics that may be harmful or too political. Today, you will be helping the moderation team by creating an algorithm that will __flag all posts not directly related to sports__.\n",
    "\n",
    "Through this lab, you will be tasked to __create an accurate machine learning algorithm__, while also questioning your own biases that may appear as you go through the lab. You will also be challenged to __think about many different edge cases__. For example, do you believe that a post harshly criticizing Colin Kapernick should be deemed as a on-topic post on Sport-It? *__And is there a right or wrong answer?__*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29478af0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12db16c",
   "metadata": {},
   "source": [
    "# Part 1: \"Naive\" Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae0598",
   "metadata": {},
   "source": [
    "## The Naive Bayes Classifiers Algorithm\n",
    "Let’s take a moment to discuss the algorithm we are going to use to properly calculate our likelihood probabilities for on-topic and off-topic posts on Sport-It. We will be using the Naive Bayes Classifiers algorithm.\n",
    "\n",
    "The Naive Bayes algorithm is a classification technique that classifies an object to a label based on prior probabilities and feature probabilities. In today’s lab, our Naive Bayes Algorithm will assign on-topic or off-topic labels to posts, depending on the probability that the words in the post would appear in either the on-topic or off-topic label.\n",
    "\n",
    "There are __two types of probabilities__ to look out for; the __prior probability__, and the __feature probability__. Let’s go over what each one means, and how to calculate them.\n",
    "\n",
    "__Prior Probability__, in this case, is the __probability that the post is an on-topic/off-topic post__. Mathematically, it would look like this: <br>\n",
    "$${P}(Label = \"y\") = \\frac{{k} + count(on topic)}{2{k} + count(posts)}\\$$\n",
    "\n",
    "__Feature Probability__, in this case, is the __probability that a specific word appears in an on-topic/off-topic label__. Mathematically, it would look like this: <br>\n",
    "$${P}(Word = \"election\" | Label = \"n\") = \\frac{{k} + count(off topic \"election\" posts)}{2{k} + count(off topic posts)}\\$$\n",
    "\n",
    "You may have noticed a constant ${k}$ appearing in the formulas above. We don’t want to run into a situation where our feature probability is 0. Adding a constant ${k}$ in the numerator and denominator fixes this issue. This technique is called [Laplace Smoothing](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece#:~:text=Laplace%20smoothing%20is%20a%20smoothing%20technique%20that%20helps%20tackle%20the,the%20positive%20and%20negative%20reviews.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b990a",
   "metadata": {},
   "source": [
    "## Using the Training Data\n",
    "We must use training data to “train” our model. We will use sample data from past posts from Sport-It, alongside pre-existing labels, to train our data. Simply put, each post will be given a label “on-topic” or “off-topic”.\n",
    "\n",
    "__Let’s start by importing any needed libraries:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import __version__\n",
    "from math import exp, log\n",
    "from lab import data_tools, display\n",
    "\n",
    "print('Content filtering lab, version', __version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad500d",
   "metadata": {},
   "source": [
    "A training dataset is already provided. Each datapoint contains __text (the post title)__, and a label __y/n (whether the post is on-topic/off-topic)__. Here is an example:\n",
    "<blockquote>y: Musgrove throws first no-hitter in Padres history.</blockquote>\n",
    "Here, the post about Musgrove is considered on-topic, as it fully pertains to a sport.\n",
    "\n",
    "Let’s start by parsing through our data and assigning the parsed data to variable *training_data*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd717b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_tools.parse_data('./data/politics.csv', './data/espn.txt', limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4327c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now that we parsed our data, we should be able to easily display our data. Using some display helper methods, we can display our training data. \n",
    "\n",
    "As you look through the dataset, ask yourself: __Do you agree with the labels provided? What would you change?__ Take a moment to discuss with your group. Remember, most edge cases have no right or wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70431971",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "display.display_labelled_data(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8ccb5",
   "metadata": {},
   "source": [
    "Notice that in the code below, each datapoint in the list passed into the extend function has a label __“y/n”. Your group should assign these labels. Discuss with your group, and decide a label for each datapoint.__ Remember, there is no right or wrong answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aee2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Assign each title in each datapoint a label \"y\" or \"n\".\n",
    "# Discuss within your group what you think the labels should be, and why.\n",
    "training_data.extend([\n",
    "    ('y/n', 'Deshaun Watson Admits Encounters with Masseuses, Always Consensual'),\n",
    "    ('y/n', 'Bruce Jenner is actually getting the Arthur Ashe Courage Award?'),\n",
    "    ('y/n', 'The real Tim Tebow: anti gay, anti choice, and a very unexceptional QB who owes a great deal to a teammate who is very good at kicking long fieldgoals when Tim cant get near the red zone.'),\n",
    "    ('y/n', 'TIL All NFL players have to do their physicals completely nude and are often nude for over an hour and a half. Many players have fears they were videotaped.'),\n",
    "    ('y/n', 'Broncos Brandon Marshall kneels during national anthem, follows Colin Kaepernick’s path'),\n",
    "    ('y/n', 'Megan Rapinoe says ‘not many, if any’ US womens soccer player’s would attend White House'),\n",
    "    ('y/n', 'With the upcoming Thursday night NFL game, remember that this presents a simplified view of an entire culture, caricatures facial features based on race, depicts an outdated/inaccurate style of headdress, paints them as warmongering aggressors and overly glamorizes the violent side of their history.')\n",
    "])\n",
    "\n",
    "display.display_labelled_data(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f61245",
   "metadata": {},
   "source": [
    "## Calculating Probabilities Using The Training Data\n",
    "Now that we parsed our data, we can now utilize it to our advantage. We will be calculating the prior probability of each label (on-topic and off-topic), and every feature probability for every word that appears in each group of posts. \n",
    "\n",
    "Recall that the prior probability is simply the probability of a certain label appearing in the training data. As a reminder, the equation looks like this:\n",
    "\n",
    "$${P}(Label = \"y\") = \\frac{{k} + count(on-topic)}{2{k} + count(posts)}\\$$\n",
    "\n",
    "Let’s go ahead and look at the _prior_probabilities_ function. __Keep in mind that ${k} = 1$.__ We have provided a helper class with methods to deal with the actual calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d792f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "training_data_statistics = data_tools.DataStats(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(label):\n",
    "    \"\"\"\n",
    "    Function input: label\n",
    "    Global/implicit input: training_data_statistics (DataStats object)\n",
    "    Output: P(Label=label)\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    num_invalid = len(training_data_statistics.invalid_posts)\n",
    "    num_valid = len(training_data_statistics.valid_posts)\n",
    "    num_total = training_data_statistics.num_posts\n",
    "    if label == 'y':\n",
    "        return (k + num_valid) / (2*k + num_total)\n",
    "    elif label == 'n':\n",
    "        return (k + num_invalid) / (2*k + num_total)\n",
    "    else:\n",
    "        raise KeyError('Unsupported label: {}'.format(label))\n",
    "print(prior_probabilities('n'))\n",
    "print(prior_probabilities('y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6a0f5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "We currently have the prior probabilities of both the on-topic and off-topic posts. In order to properly label posts on Sport-It as on-topic or off-topic, we also need to calculate the feature probabilities. That is, __the probability of each word appearing in each group of posts; on-topic and off-topic.__\n",
    "\n",
    "Let’s use the _word_given_label_probability_ function, that will go through each word in each post in each label, and calculate the feature probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2f7f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def word_given_label_probability(word, label):\n",
    "    \"\"\"\n",
    "    Function input: label\n",
    "    Global/implicit input: training_data_statistics (DataStats object)\n",
    "    Output: P(word | Label=label)\n",
    "    \"\"\"\n",
    "    if label == 'y':\n",
    "        return training_data_statistics.valid_counter[word] / training_data_statistics.total_invalid_words\n",
    "    elif label == 'n':\n",
    "        return training_data_statistics.invalid_counter[word] / training_data_statistics.total_invalid_words\n",
    "    else:\n",
    "        raise KeyError('Unsupported label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a11c9",
   "metadata": {},
   "source": [
    "## Predicting Labels for Posts\n",
    "Time to see if our algorithm and model work! We now have the prior probabilities, as well as all the feature probabilities. We can correctly predict what label a post should have. For example if we were given the following post:\n",
    "<blockquote>The game last night was absolutely terrible!</blockquote>\n",
    "\n",
    "We should be able to __determine this post's label by the likelihood score of it being on-topic and off-topic__:\n",
    "\n",
    "<blockquote>${P}$(Label='<font color='green'>y</font>'|AllWords) = ${P}$(Label='<font color='green'>y</font>') $* {P}$(Word='<font color='blue'>The</font>'|Label='<font color='green'>y</font>') $* {P}$(Word='<font color='blue'>game</font>'|Label='<font color='green'>y</font>') $* {P}$(Word='<font color='blue'>last</font>'|Label='<font color='green'>y</font>') $*$ ...</blockquote>\n",
    "\n",
    "Once you have the probability of the post being on-topic or off-topic given the words in the posts, __choose the label corresponding to the highest probability of the two to correctly classify that post__. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1d497",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Create a function _post_validity_ that __returns the predicted label assigned to the submission post__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa47f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_validity(submission, threshold = 0):\n",
    "    \"\"\"\n",
    "    Input: A particular submission (title of a post)\n",
    "    Threshold: a threshold for tuning to mark for manual review\n",
    "    \"\"\"\n",
    "    word_arr = data_tools.preprocess_submission(submission)\n",
    "    sum_log_word_given_valid = 0\n",
    "    sum_log_word_given_invalid = 0\n",
    "    for word in word_arr:\n",
    "        word_given_y = word_given_label_probability(word, 'y')\n",
    "        word_given_n = word_given_label_probability(word, 'n')\n",
    "        if word_given_y > 0:\n",
    "            sum_log_word_given_valid += log(word_given_y)\n",
    "        if word_given_n > 0:\n",
    "            sum_log_word_given_invalid += log(word_given_n)\n",
    "    log_ratio = log(prior_probabilities('y')) - log(prior_probabilities('n')) + sum_log_word_given_valid - sum_log_word_given_invalid\n",
    "    if log_ratio < -1 * threshold:\n",
    "        return 'n'\n",
    "    elif log_ratio > -1 * threshold:\n",
    "        return 'y'\n",
    "    else:\n",
    "        # manual review\n",
    "        return '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558139b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The code below returns an array of tuples(labels, titles) based on the probabilities we found before. __With your group, quickly discuss the code and what it does__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab.data_tools import parse_unlabeled_reddit_feed, parse_unlabeled_espn\n",
    "espn_data = parse_unlabeled_espn('./data/test/espn.txt', limit=100)\n",
    "politics_data = parse_unlabeled_reddit_feed('./data/test/politics.txt', limit=100)\n",
    "\n",
    "testing_data = espn_data + politics_data\n",
    "solution = [('y', e) for e in espn_data] + [('n', p) for p in politics_data]\n",
    "\n",
    "def filter_posts(posts):\n",
    "    \"\"\"\n",
    "    Input: array of posts to filter WITHOUT labels (see output of parse_unlabeled_espn/reddit_feed)\n",
    "    Output: array of posts as tuples (label, post title), see output of parse_data\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    result = []\n",
    "    for submission in testing_data:\n",
    "        validity = post_validity(submission)\n",
    "        result.append((validity, submission))\n",
    "\n",
    "    return result\n",
    "filtering_result = filter_posts(testing_data)\n",
    "display.display_labelled_data(filtering_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05863587",
   "metadata": {},
   "source": [
    "Great! Now we predicted labels for the posts in the testing set, based on the probabilities we calculated. Let’s go ahead and test how accurate your algorithm is! Run the code below to get a percentage of labels you correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08304e69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def verify_algorithm(test_result, solution):\n",
    "    \"\"\"\n",
    "    Input: result of the test labelling, and the solution labelling. They should both be arrays of tuples (see output of parse_data for info)\n",
    "    Output: Entries in test_result that did not appear in solution -- also known as wrong entries\n",
    "    \"\"\"\n",
    "    return list(set(test_result) - set(solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabelled = verify_algorithm(filtering_result, solution)\n",
    "score = (1 - (len(mislabelled) / len(solution)))\n",
    "print('Your accuracy is:', 100*score,'%')\n",
    "display.display_labelled_data(mislabelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75910086",
   "metadata": {},
   "source": [
    "Look at the percentage you got when you ran the code block above. Were you expecting this score? Modify your past code, and see how the percentage changes. Try to get the best possible percentage. __Remember, incorrect moderation has real consequences for Sport-It.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc3d7f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ca793",
   "metadata": {},
   "source": [
    "# Part 2: The Consequences of Naivety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903c183",
   "metadata": {},
   "source": [
    "### __More about the effects of a Naive content moderation algorithm can be found on the [ethical moderation website](https://dylanirlbeck.github.io/ethical-moderation/project#part-2-the-consequences-of-naivety)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5659e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8c0e7",
   "metadata": {},
   "source": [
    "# Part 3: Introducing Human Content Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be621b16",
   "metadata": {},
   "source": [
    "### __More about human content moderation can be found on the [ethical moderation website](https://dylanirlbeck.github.io/ethical-moderation/project#part-3-introducing-human-content-moderation)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605c2fd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54630a",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498cf1e",
   "metadata": {},
   "source": [
    " - [At the End of the Day Facebook Does What it Wants](https://s3.amazonaws.com/kvaccaro.com/documents/vaccaro_cscw2020.pdf)\n",
    " - [Introduction to Bag-Of-Words Models](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)\n",
    " - [Introduction to NBC](https://towardsdatascience.com/introduction-to-naive-bayes-classification-4cffabb1ae54)\n",
    " - [Laplace Smoothing](https://towardsdatascience.com/laplace-smoothing-in-na%C3%AFve-bayes-algorithm-9c237a8bdece#:~:text=Laplace%20smoothing%20is%20a%20smoothing%20technique%20that%20helps%20tackle%20the,the%20positive%20and%20negative%20reviews.)\n",
    " - [Sample Data Examples](https://www.reddit.com/r/sports/controversial)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
