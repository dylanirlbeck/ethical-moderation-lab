{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2366b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab libraries imported!\n",
      "Content filtering lab, version 0.1\n"
     ]
    }
   ],
   "source": [
    "from lab import __version__\n",
    "from math import exp, log\n",
    "\n",
    "print('Content filtering lab, version', __version__)\n",
    "\n",
    "# ** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76939895",
   "metadata": {},
   "source": [
    "# Ethical Moderation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77599c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Content Moderation is a subtopic within Computer Science ethics that has gained traction since the rise of popular social media platforms. Successful platforms such as Twitter, Reddit, and Quora have bred a space where everyone is allowed to voice their opinions on any topic they please.\n",
    "\n",
    "Today, we will explore content moderation in a Computer Science perspective, and the delicate issues that arise from too much or too little moderation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ede49f",
   "metadata": {},
   "source": [
    "## Lab Background\n",
    "Sport-It is a popular social media platform which is known for its variety of communities, all of which relate to a specific sport. For example, there is a Sport-It community for the NBA, NFL, NHL, and many more. \n",
    "\n",
    "The moderators at Sport-It have decided that they want to create an environment where users only post about sports, and not controversial topics that may be harmful or too political. Today, you will be helping the moderation team by creating an algorithm that will __flag all posts not directly related to sports__.\n",
    "\n",
    "Through this lab, you will be tasked to __create an accurate machine learning algorithm__, while also questioning your own biases that may appear as you go through the lab. You will also be challenged to __think about many different edge cases__. For example, do you believe that a post harshly criticizing Colin Kapernick should be deemed as a valid post on Sport-It? *__And is there a right or wrong answer?__*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12db16c",
   "metadata": {},
   "source": [
    "# 1: Automated Content Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae0598",
   "metadata": {},
   "source": [
    "## The Naive Bayes Algorithm\n",
    "Let’s take a moment to discuss the algorithm we are going to use to properly calculate our likelihood probabilities for valid and invalid posts on Sport-It. We will be using the Naive Bayes Classifiers algorithm.\n",
    "### What does the Algorithm do?\n",
    "The Naive Bayes algorithm is a classification technique that classifies an object to a label based on prior probabilities and feature probabilities. In today’s lab, our Naive Bayes Algorithm will assign valid or invalid labels to posts, depending on the probability that the words in the post would appear in either the valid or invalid label.\n",
    "### Calculating the Probabilities\n",
    "There are __two types of probabilities__ to look out for; the __prior probability__, and the __feature probability__. Let’s go over what each one means, and how to calculate them.\n",
    "\n",
    "__Prior Probability__, in this case, is the __probability that the post is a valid/invalid post__. Mathematically, it would look like this: <br>\n",
    "$${P}(Label = \"on topic\") = \\frac{{k} + count(on topic posts)}{2{k} + count(posts)}\\$$\n",
    "\n",
    "__Feature Probability__, in this case, is the __probability that a specific word appears in a on-topic/off-topic label__. Mathematically, it would look like this: <br>\n",
    "$${P}(Word = \"election\" | Label = \"off topic\") = \\frac{{k} + count(off topic \"election\" posts)}{2{k} + count(off topic posts)}\\$$\n",
    "\n",
    "You may have noticed a constant k appearing in the formulas above. We don’t want to run into a situation where our feature probability is 0. Adding a constant k in the numerator and denominator fixes this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b990a",
   "metadata": {},
   "source": [
    "## Using the Training Data\n",
    "We must use training data to “train” our model. We will use sample data from past posts from Sport-It, alongside pre-existing labels, to train our data. Simply put, each post will be given a label “on-topic” or “off-topic”.\n",
    "\n",
    "__Let’s start by importing any needed libraries:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040110e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import data_tools, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad500d",
   "metadata": {},
   "source": [
    "A training dataset is already provided. Each datapoint contains __text (the post title)__, and a label __y/n (whether the post is on-topic or off-topic)__. Here is an example:\n",
    "<blockquote>y: Musgrove throws first no-hitter in Padres history.</blockquote>*\n",
    "Here, the post about Musgrove is considered on-topic, as it fully pertains to a sport.\n",
    "\n",
    "As you look through the dataset, ask yourself: *__Do you agree with the labels provided? What would you change?__* Take a moment to discuss with your group. Remember, most edge cases have no right or wrong answer.\n",
    "\n",
    "Let’s start by creating a function to parse through our dataset. Given a file, return an array of tuples: __END POINT HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd717b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_tools.parse_data('./data/politics.csv', './data/espn.txt', limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4327c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Code block that displays data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70431971",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "display.display_labelled_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aee2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more edge case examples for students to explore and discuss, between 10 and 20 in total\n",
    "training_data.extend([\n",
    "    ('y/n', 'A passable title'),\n",
    "    ('y/n', 'With the upcoming Thursday night NFL game, remember that this presents a simplified view of an entire culture, caricatures facial features based on race, depicts an outdated/inaccurate style of headdress, paints them as warmongering aggressors and overly glamorizes the violent side of their history.')\n",
    "])\n",
    "\n",
    "display.display_labelled_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d792f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# We will use a helper class to help the students deal with the actual calculation (so they can focus on ethics and not programming)\n",
    "training_data_statistics = data_tools.DataStats(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba39af",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Code where they implement prior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(label):\n",
    "    \"\"\"\n",
    "    Function input: label\n",
    "    Global/implicit input: training_data_statistics (DataStats object)\n",
    "    Output: P(Label=label)\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    num_invalid = len(training_data_statistics.invalid_posts)\n",
    "    num_valid = len(training_data_statistics.valid_posts)\n",
    "    num_total = training_data_statistics.num_posts\n",
    "    if label == 'y':\n",
    "        return (k + num_valid) / (2*k + num_total)\n",
    "    elif label == 'n':\n",
    "        return (k + num_invalid) / (2*k + num_total)\n",
    "    else:\n",
    "        raise KeyError('Unsupported label: {}'.format(label))\n",
    "print(prior_probabilities('n'))\n",
    "print(prior_probabilities('y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6a0f5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Code where they implement feature probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2f7f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def word_given_label_probability(word, label):\n",
    "    \"\"\"\n",
    "    Function input: label\n",
    "    Global/implicit input: training_data_statistics (DataStats object)\n",
    "    Output: P(word | Label=label)\n",
    "    \"\"\"\n",
    "    if label == 'y':\n",
    "        return training_data_statistics.valid_counter[word] / training_data_statistics.total_invalid_words\n",
    "    elif label == 'n':\n",
    "        return training_data_statistics.invalid_counter[word] / training_data_statistics.total_invalid_words\n",
    "    else:\n",
    "        raise KeyError('Unsupported label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ba731",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Code where they test the probability of the post being valid and the probability of the post being invalid\n",
    "Returns a tuple: (p_valid, p_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd323b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def submission_probabilities(submission, label):\n",
    "    # TODO: !!! Might be obsolete\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1d497",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Code that returns the maximum of the probability of being valid and invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa47f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_validity(submission, threshold = 0):\n",
    "    \"\"\"\n",
    "    Input: A particular submission (title of a post)\n",
    "    Threshold: a threshold for tuning to mark for manual review\n",
    "    \"\"\"\n",
    "    # TODO: Is this log calculation problematic?\n",
    "    word_arr = data_tools.preprocess_submission(submission)\n",
    "    sum_log_word_given_valid = 0\n",
    "    sum_log_word_given_invalid = 0\n",
    "    for word in word_arr:\n",
    "        word_given_y = word_given_label_probability(word, 'y')\n",
    "        word_given_n = word_given_label_probability(word, 'n')\n",
    "        if word_given_y > 0:\n",
    "            sum_log_word_given_valid += log(word_given_y)\n",
    "        if word_given_n > 0:\n",
    "            sum_log_word_given_invalid += log(word_given_n)\n",
    "    log_ratio = log(prior_probabilities('y')) - log(prior_probabilities('n')) + sum_log_word_given_valid - sum_log_word_given_invalid\n",
    "    # TODO: Maybe assert threshhold is positive for this to work\n",
    "    if log_ratio < -1 * threshold:\n",
    "        return 'n'\n",
    "    elif log_ratio > -1 * threshold:\n",
    "        return 'y'\n",
    "    else:\n",
    "        # TODO: Maybe this isn't the symbol you want?\n",
    "        return '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733bb80",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "(UNOFFICIAL) Code that compiles all of the students' functions into a single model object generator thing\n",
    "It should also process the actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558139b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Code that returns the array of tuples(label, title) based on the probabilities that we found before\n",
    "Input: testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab.data_tools import parse_unlabeled_reddit_feed, parse_unlabeled_espn\n",
    "espn_data = parse_unlabeled_espn('./data/test/espn.txt', limit=100)\n",
    "politics_data = parse_unlabeled_reddit_feed('./data/test/politics.txt', limit=100)\n",
    "\n",
    "testing_data = espn_data + politics_data\n",
    "solution = [('y', e) for e in espn_data] + [('n', p) for p in politics_data]\n",
    "\n",
    "def filter_posts(posts):\n",
    "    \"\"\"\n",
    "    Input: array of posts to filter WITHOUT labels (see output of parse_unlabeled_espn/reddit_feed)\n",
    "    Output: array of posts as tuples (label, post title), see output of parse_data\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    result = []\n",
    "    for submission in testing_data:\n",
    "        validity = post_validity(submission)\n",
    "        result.append((validity, submission))\n",
    "\n",
    "    return result\n",
    "filtering_result = filter_posts(testing_data)\n",
    "display.display_labelled_data(filtering_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a1827",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Code block that returns the percentage of labels they predicted correctly\n",
    "This should *just work*, e.g. it should already be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08304e69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate percent correctness - could just do by calculating len(verify_algorithm(test_result, solution)) / len(solution)\n",
    "def verify_algorithm(test_result, solution):\n",
    "    \"\"\"\n",
    "    Input: result of the test labelling, and the solution labelling. They should both be arrays of tuples (see output of parse_data for info)\n",
    "    Output: Entries in test_result that did not appear in solution -- also known as wrong entries\n",
    "    \"\"\"\n",
    "    return list(set(test_result) - set(solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabelled = verify_algorithm(filtering_result, solution)\n",
    "# TODO: Alter number of test cases (there are a lot of mistakes so far, I think)\n",
    "score = (1 - (len(mislabelled) / len(solution)))\n",
    "print('Your accuracy is:', 100*score,'%')\n",
    "display.display_labelled_data(mislabelled)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
