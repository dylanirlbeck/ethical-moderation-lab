{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67450e95",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "This file will be converted to an actual notebook later\n",
    "This file may be used to define high-level functions that may warrant discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2366b2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from lab import __version__\n",
    "from math import exp, log\n",
    "\n",
    "print('Content filtering lab, version', __version__)\n",
    "\n",
    "# ** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d8c12",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Code block that contains the importing of the library. \n",
    "It will already be there, they just have to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import data_tools, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efb90a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Code block that parses data into the program\n",
    "Put in the labels your group thinks is appropriate for the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd717b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_tools.parse_data('./data/politics.csv', './data/espn.txt', limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4327c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Code block that displays data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70431971",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "display.display_labelled_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aee2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add more edge case examples for students to explore and discuss, between 10 and 20 in total\n",
    "training_data.extend([\n",
    "    ('y/n', 'A passable title'),\n",
    "    ('y/n', 'With the upcoming Thursday night NFL game, remember that this presents a simplified view of an entire culture, caricatures facial features based on race, depicts an outdated/inaccurate style of headdress, paints them as warmongering aggressors and overly glamorizes the violent side of their history.')\n",
    "])\n",
    "\n",
    "display.display_labelled_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d792f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# We will use a helper class to help the students deal with the actual calculation (so they can focus on ethics and not programming)\n",
    "training_data_statistics = data_tools.DataStats(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba39af",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Code where they implement prior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(label):\n",
    "    \"\"\"\n",
    "    Function input: label\n",
    "    Global/implicit input: training_data_statistics (DataStats object)\n",
    "    Output: P(Label=label)\n",
    "    \"\"\"\n",
    "    k = 1\n",
    "    num_invalid = len(training_data_statistics.invalid_posts)\n",
    "    num_valid = len(training_data_statistics.valid_posts)\n",
    "    num_total = training_data_statistics.num_posts\n",
    "    if label == 'y':\n",
    "        return (k + num_valid) / (2*k + num_total)\n",
    "    elif label == 'n':\n",
    "        return (k + num_invalid) / (2*k + num_total)\n",
    "    else:\n",
    "        raise KeyError('Unsupported label: {}'.format(label))\n",
    "print(prior_probabilities('n'))\n",
    "print(prior_probabilities('y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6a0f5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Code where they implement feature probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2f7f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def word_given_label_probability(word, label):\n",
    "    \"\"\"\n",
    "    Function input: label\n",
    "    Global/implicit input: training_data_statistics (DataStats object)\n",
    "    Output: P(word | Label=label)\n",
    "    \"\"\"\n",
    "    if label == 'y':\n",
    "        return training_data_statistics.valid_counter[word] / training_data_statistics.total_invalid_words\n",
    "    elif label == 'n':\n",
    "        return training_data_statistics.invalid_counter[word] / training_data_statistics.total_invalid_words\n",
    "    else:\n",
    "        raise KeyError('Unsupported label: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ba731",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Code where they test the probability of the post being valid and the probability of the post being invalid\n",
    "Returns a tuple: (p_valid, p_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd323b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def submission_probabilities(submission, label):\n",
    "    # TODO: !!! Might be obsolete\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1d497",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Code that returns the maximum of the probability of being valid and invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa47f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_validity(submission, threshold = 0):\n",
    "    \"\"\"\n",
    "    Input: A particular submission (title of a post)\n",
    "    Threshold: a threshold for tuning to mark for manual review\n",
    "    \"\"\"\n",
    "    # TODO: Is this log calculation problematic?\n",
    "    word_arr = data_tools.preprocess_submission(submission)\n",
    "    sum_log_word_given_valid = 0\n",
    "    sum_log_word_given_invalid = 0\n",
    "    for word in word_arr:\n",
    "        word_given_y = word_given_label_probability(word, 'y')\n",
    "        word_given_n = word_given_label_probability(word, 'n')\n",
    "        if word_given_y > 0:\n",
    "            sum_log_word_given_valid += log(word_given_y)\n",
    "        if word_given_n > 0:\n",
    "            sum_log_word_given_invalid += log(word_given_n)\n",
    "    log_ratio = log(prior_probabilities('y')) - log(prior_probabilities('n')) + sum_log_word_given_valid - sum_log_word_given_invalid\n",
    "    # TODO: Maybe assert threshhold is positive for this to work\n",
    "    if log_ratio < -1 * threshold:\n",
    "        return 'n'\n",
    "    elif log_ratio > -1 * threshold:\n",
    "        return 'y'\n",
    "    else:\n",
    "        # TODO: Maybe this isn't the symbol you want?\n",
    "        return '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733bb80",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "(UNOFFICIAL) Code that compiles all of the students' functions into a single model object generator thing\n",
    "It should also process the actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558139b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Code that returns the array of tuples(label, title) based on the probabilities that we found before\n",
    "Input: testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab.data_tools import parse_unlabeled_reddit_feed, parse_unlabeled_espn\n",
    "espn_data = parse_unlabeled_espn('./data/test/espn.txt', limit=100)\n",
    "politics_data = parse_unlabeled_reddit_feed('./data/test/politics.txt', limit=100)\n",
    "\n",
    "testing_data = espn_data + politics_data\n",
    "solution = [('y', e) for e in espn_data] + [('n', p) for p in politics_data]\n",
    "\n",
    "def filter_posts(posts):\n",
    "    \"\"\"\n",
    "    Input: array of posts to filter WITHOUT labels (see output of parse_unlabeled_espn/reddit_feed)\n",
    "    Output: array of posts as tuples (label, post title), see output of parse_data\n",
    "    \"\"\"\n",
    "    # your code here!\n",
    "    result = []\n",
    "    for submission in testing_data:\n",
    "        validity = post_validity(submission)\n",
    "        result.append((validity, submission))\n",
    "\n",
    "    return result\n",
    "filtering_result = filter_posts(testing_data)\n",
    "display.display_labelled_data(filtering_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a1827",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Code block that returns the percentage of labels they predicted correctly\n",
    "This should *just work*, e.g. it should already be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08304e69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate percent correctness - could just do by calculating len(verify_algorithm(test_result, solution)) / len(solution)\n",
    "def verify_algorithm(test_result, solution):\n",
    "    \"\"\"\n",
    "    Input: result of the test labelling, and the solution labelling. They should both be arrays of tuples (see output of parse_data for info)\n",
    "    Output: Entries in test_result that did not appear in solution -- also known as wrong entries\n",
    "    \"\"\"\n",
    "    return list(set(test_result) - set(solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabelled = verify_algorithm(filtering_result, solution)\n",
    "# TODO: Alter number of test cases (there are a lot of mistakes so far, I think)\n",
    "score = (1 - (len(mislabelled) / len(solution)))\n",
    "print('Your accuracy is:', 100*score,'%')\n",
    "display.display_labelled_data(mislabelled)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
